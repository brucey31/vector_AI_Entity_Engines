{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a8846ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> div#notebook-container { width: 95%; } </style> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "__author__ = \"Bruce Pannaman\"\n",
    "\n",
    "import warnings\n",
    "from IPython.display import display, HTML\n",
    "warnings.simplefilter('ignore')\n",
    "display(HTML(data=\"\"\"<style> div#notebook-container { width: 95%; } </style> \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4bfee0",
   "metadata": {},
   "source": [
    "<p>Before running this script please do the following steps:</p><ol><li>pip install -r requirements.txt</li><li>python3 -m spacy download en_core_web_sm</li><li>brew install enchant</li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c99223",
   "metadata": {},
   "source": [
    "## Initial thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e4512a",
   "metadata": {},
   "source": [
    "<p>After reading the document, my understanding is that the following functionality is needed:</p><ol><li>Categorise each short string or phrase into the following categories.</li><ul><li>Company Names</li><li>Company Addresses</li><li>Serial Numbers</li><li>Physical Goods</li><li>Locations</li></ul><li>Establish links between each string within each category to ascertain if they represent the same entity</li><li>Group and concatonate the entities within each category</li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4260a2",
   "metadata": {},
   "source": [
    "## Questions for the wider Vector.ai team"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64fea17",
   "metadata": {},
   "source": [
    "<p>Priorities:</p><ol><li>Who are the primary and subsequent users of the output of this engine?</li><li>Is accuracy more important than speed?</li><li>Is there a particilar format/ storage option that this engine can be outputted into e.g. API endpoint, DB entry or module to be used in wider codebase.</li></ol>\n",
    "<p>Assumptions to be questioned:</p><ol><li>Are all of these documents in English?</li><li>How have these stings been obtained and is there a chance of underlying errors e.g. have these strings been taken from scanned images using OCR techniques?</li><li>For it's use case within vector.ai, should the model focus on precision or recall when grouping entities?</li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b064e18b",
   "metadata": {},
   "source": [
    "## Calculating Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb543cec",
   "metadata": {},
   "source": [
    "<p>To recognise locations, Spacy has a really good named entity model that will give us an accurate reading as to whether the string contains a location. This model will return a result of \"LOC\" or \"GPE\" denoting Location or Geopolitical entity. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae72c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# python3 -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d207ca4",
   "metadata": {},
   "source": [
    "<p>To gain more granularity on the location once we have established that there is a location or address in the field. There is a library called <a href=\"https://github.com/somnathrakshit/geograpy3\">geograpy3</a> that can do the heavy lifting of this for us to extract more data from the location string.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3ce0212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/bruce/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/bruce/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/bruce/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/bruce/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/bruce/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geograpy\n",
    "# This library uses the NLTK library and the following models to operate.\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a41dbb1",
   "metadata": {},
   "source": [
    "<p>If there is a postcode or zipcode, a lot of data can be extracted from just this part. There is another library called <a href=\"https://pypi.org/project/pgeocode/\">pgeocode</a>, which can extract more data from a postcode or zipcode if the country can be given. I will use regex to extract the postcode/ zipcode and run it against the pgeocode library for my primary location information</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dfdd2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pgeocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c4d1213",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_locations = [\"SLOUGH, SE12 2XY\", \"33 TIMBER YARD, LONDON, L1 8XY\", \"44 CHINA ROAD, KOWLOON, HONG KONG\", \"Scott House, Suite 1, The Concourse, Waterloo Station, London, England, SE1 7LY\", \"LONDON\", \"HONG KONG\", \"ASIA\", \"Nevereverland\", \"Planet Saturn\", \"Jurassic Park\", \"Tatooine\", \"Nutella Sandwich\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "057082d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationEngine:\n",
    "    def __init__(self, search_string):\n",
    "        self.search_string = search_string\n",
    "        self.contains_locations = False\n",
    "        \n",
    "        # These will be the granularity lists to be returned from the engine\n",
    "        self.location_contexts = [\"countries\", \"regions\"]\n",
    "        # Create lists to collect each location type\n",
    "        for location_context in self.location_contexts:\n",
    "            exec(\"self.{LC}_list = []\".format(LC=location_context))\n",
    "            \n",
    "    def clean_string(self, string):\n",
    "        \"\"\"Cleans the string up to allow for better NLTK operations\"\"\"\n",
    "        return string.strip().lower()\n",
    "    \n",
    "    def locations_found(self):\n",
    "        \"\"\"Returns if there is a location found within the string\"\"\"\n",
    "        found_locations = []\n",
    "        doc = nlp(self.search_string)\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in [\"LOC\", \"GPE\"]:\n",
    "                found_locations.append(ent.text)\n",
    "                \n",
    "        return len(found_locations) > 0, found_locations\n",
    "            \n",
    "    def check_zip_code_regex(self, string):\n",
    "        \"\"\"\n",
    "        Checks the string against a dict of world zipcode array to establish if\n",
    "        there is a match for an address identifier\n",
    "        \"\"\"\n",
    "        # More can be added for all 195 countries in the world if needed\n",
    "        postcode_regex_dict = {\"gb\": \"([Gg][Ii][Rr] 0[Aa]{2})|((([A-Za-z][0-9]{1,2})|(([A-Za-z][A-Ha-hJ-Yj-y][0-9]{1,2})|(([A-Za-z][0-9][A-Za-z])|([A-Za-z][A-Ha-hJ-Yj-y][0-9][A-Za-z]?))))\\\\s?[0-9][A-Za-z]{2})\", \"us\": r'.*(\\d{5}(\\-\\d{4})?)$'}\n",
    "        # Search each countries address identifier's regex to find a match\n",
    "        for country_code in postcode_regex_dict:\n",
    "            result = re.findall(postcode_regex_dict[country_code], string)\n",
    "            if len(result) > 0:\n",
    "                return country_code\n",
    "        return None\n",
    "\n",
    "    def lookup_zipcode(self, country_code, zipcode):\n",
    "        \"\"\"Uses the pgeocode library to gather further information about the location\"\"\"\n",
    "        db = pgeocode.Nominatim(country_code)\n",
    "        result = db.query_postal_code(zipcode)\n",
    "        return result.country_code, result.state_name\n",
    "        \n",
    "    def identify_location(self):\n",
    "        \"\"\"\n",
    "        Goes through each line of the string to search for clues as to whether this is a location or not\n",
    "        ASSUMPTION - locations & addresses are comma delimited\n",
    "        \n",
    "        \"\"\"\n",
    "        # Reverse the list as the strongest signals of a location are at the end e.g. countries or postcodes\n",
    "        locations_found, location_strings = self.locations_found()\n",
    "        \n",
    "        self.places_list = location_strings\n",
    "        self.contains_locations = locations_found\n",
    "        for line in self.search_string.split(\",\")[::-1]:\n",
    "            cleaned_string = self.clean_string(line)\n",
    "\n",
    "            # Check if the substring has a match with a zipcode or postcode format\n",
    "            zip_code_country = self.check_zip_code_regex(cleaned_string)\n",
    "            # If there is a match with the zipcode regex, focus in on that instead of string representations\n",
    "            if zip_code_country is not None:\n",
    "                self.contains_locations = True\n",
    "                results = self.lookup_zipcode(zip_code_country, cleaned_string)\n",
    "                for i, result in enumerate(results):\n",
    "                    exec(\"self.{LC}_list = [result]\".format(LC=self.location_contexts[i]))\n",
    "\n",
    "            else:   \n",
    "                places = geograpy.get_geoPlace_context(text=cleaned_string.capitalize())\n",
    "                for location_context in self.location_contexts:\n",
    "                    results = eval(\"places.{LC}\".format(LC=location_context))\n",
    "                    if len(results) > 0:\n",
    "                        self.contains_locations = True\n",
    "                        exec(\"self.{LC}_list.append(places.{LC}[0])\".format(LC=location_context))\n",
    "                        \n",
    "        return self.contains_locations, {\"countries\": self.countries_list, \"regions\": self.regions_list, \"places\": self.places_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb421e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLOUGH, SE12 2XY\n",
      "Location found in string = True\n",
      "Locations found = {'countries': ['GB', 'United Kingdom'], 'regions': ['England'], 'places': []}\n",
      "\n",
      "33 TIMBER YARD, LONDON, L1 8XY\n",
      "Location found in string = True\n",
      "Locations found = {'countries': ['GB', 'United Kingdom'], 'regions': ['England'], 'places': ['LONDON']}\n",
      "\n",
      "44 CHINA ROAD, KOWLOON, HONG KONG\n",
      "Location found in string = True\n",
      "Locations found = {'countries': ['Hong Kong'], 'regions': [], 'places': ['HONG KONG']}\n",
      "\n",
      "Scott House, Suite 1, The Concourse, Waterloo Station, London, England, SE1 7LY\n",
      "Location found in string = True\n",
      "Locations found = {'countries': ['GB', 'United States', 'United Kingdom', 'Australia', 'United States'], 'regions': ['England'], 'places': ['London', 'England']}\n",
      "\n",
      "LONDON\n",
      "Location found in string = True\n",
      "Locations found = {'countries': ['United Kingdom'], 'regions': [], 'places': ['LONDON']}\n",
      "\n",
      "HONG KONG\n",
      "Location found in string = True\n",
      "Locations found = {'countries': [], 'regions': [], 'places': ['HONG KONG']}\n",
      "\n",
      "ASIA\n",
      "Location found in string = True\n",
      "Locations found = {'countries': [], 'regions': [], 'places': ['ASIA']}\n",
      "\n",
      "Nevereverland\n",
      "Location found in string = True\n",
      "Locations found = {'countries': [], 'regions': [], 'places': ['Nevereverland']}\n",
      "\n",
      "Planet Saturn\n",
      "Location found in string = False\n",
      "Locations found = {'countries': [], 'regions': [], 'places': []}\n",
      "\n",
      "Jurassic Park\n",
      "Location found in string = True\n",
      "Locations found = {'countries': [], 'regions': [], 'places': ['Jurassic Park']}\n",
      "\n",
      "Tatooine\n",
      "Location found in string = False\n",
      "Locations found = {'countries': [], 'regions': [], 'places': []}\n",
      "\n",
      "Nutella Sandwich\n",
      "Location found in string = False\n",
      "Locations found = {'countries': [], 'regions': [], 'places': []}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for example_location in example_locations:\n",
    "    location_engine = LocationEngine(example_location)\n",
    "    location_found, locations = location_engine.identify_location()\n",
    "    print(example_location)\n",
    "    print(\"Location found in string = %s\" % str(location_found))\n",
    "    print(\"Locations found = %s\" % str(locations))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f070286",
   "metadata": {},
   "source": [
    "<p>As you can see here we now have an engine that will be able to take information from a string and ascertain whether there is a location within it or not. It is not perfect in any strech of the imagination as it misses out major location names such as Hong Kong or Asia</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe8dd6c",
   "metadata": {},
   "source": [
    "## Calculating Physical Goods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b49a3c",
   "metadata": {},
   "source": [
    "<p>For this category, there will have to be something a little different to establish if:</p><ol><li>The string is referencing an object at all</li><li>If the physical good is something that is a collective item or something that is a one off</li></ol><br><p>One approach that would work for the examples would be to use the NLTK parts of speech model (POS) to look for words next to each other that are singular nouns. The Spacy entity recognition model doesn't work very well here.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0dc90a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_physical_goods = [\"HARDWOOD TABLE\", \"Frontloaded Washing Machine\", \"United States\", \"PLASTIC BOTTLE\", \"Jesus Christ\", \"Bruce Pannaman\", \"Swimmingly Fantastic\", \"Recipe Book\", \"Once upon a time, a cat ran up the hill\", \"White Kitchen table with tiny little handles on the side\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8c62bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def physical_goods_engine(string):\n",
    "    \"\"\"Checks for named pronouns next to each other and returns a list of the physical objects found\"\"\"\n",
    "    physical_objects_found = []\n",
    "    tokens = nltk.word_tokenize(string)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    # calculate the parts of speech for every word in the string\n",
    "    tagged = {k:v for k,v in nltk.pos_tag(tokens)}\n",
    "    for i, word in enumerate(tagged):\n",
    "        if i > 0:\n",
    "            # if there are two nouns in a row, there is a high change that there is a physical good such as the ones in the examples.\n",
    "            if tagged[word] == \"NN\" and list(tagged.values())[i-1] == \"NN\":\n",
    "                physical_objects_found.append(\" \".join([list(tagged.keys())[i-1], list(tagged.keys())[i]]))\n",
    "                \n",
    "    return len(physical_objects_found) > 0, physical_objects_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f4c21ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, ['hardwood table'])\n",
      "(True, ['washing machine'])\n",
      "(False, [])\n",
      "(True, ['plastic bottle'])\n",
      "(True, ['jesus christ'])\n",
      "(True, ['bruce pannaman'])\n",
      "(False, [])\n",
      "(True, ['recipe book'])\n",
      "(False, [])\n",
      "(True, ['kitchen table'])\n"
     ]
    }
   ],
   "source": [
    "for example_physical_good in example_physical_goods:\n",
    "    print(physical_goods_engine(example_physical_good))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a4c81b",
   "metadata": {},
   "source": [
    "<p>This approach works remarkably well for physical goods with 2 words in them. It does have some errors though:</p><ol><li>It cannot recognise the difference between names and objects. This coiuld be negated by cross referencing it against a dataset of Person names to rule this out, however, it will not be fool proof e.g. for something like a designer furniture called the \"Rebecca Chair\"</li><li>This approach is limited to looking at Bi-grams, where as a physical good can be more or less than 1 word.</li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac180d15",
   "metadata": {},
   "source": [
    "<p>If there was a third party API that could be used to verify physical good categories, this would improve performance significantly. I would suggest something like the Amazon products api, or a similar version from Aliexpress or Ebay. I would use the API to search for the term and assess whether there is a physical good based on the number of items available to buy that come back.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1967ad",
   "metadata": {},
   "source": [
    "## Serial Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aafed70",
   "metadata": {},
   "source": [
    "<p>When identifying serial numbers, I would suggest that the main determination will be whether the string contains non-nouns of which are not in the dictionary. <br><br>To do this I will use the NLTK parts of speech model combined with a library called <a href=\"https://pypi.org/project/pyenchant/\">Pyenchant</a> to assess if the word is in the dictionary or not. If a third or more of the tokens in a phrase are not nouns and also not in the dictionary, the string is more than likely fits into the serial number category</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15d295e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "# brew install enchant  \n",
    "dictionary = enchant.Dict(\"en_GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "173b59c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_serial_numbers = [\"XYZ 13423 / ILD\", \"ABC/ICL/20891NC\", \"stc112hJJ5\", \"xn256 1jj\", \"Smart Camel\", \"Lemon Pancake\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f757896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serial_number_engine(string):\n",
    "    \"\"\"Assesses whether the string is a serial number based on NLTK pos model and the enchant library to check if words are in the dictionary\"\"\"\n",
    "    tokens = re.split('/|\\W',string)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tagged = {k:v for k,v in nltk.pos_tag(tokens) if len(k) > 0}\n",
    "    # If there is only one word, just check if it is in the dictionary using the enchant Library\n",
    "    if len(tagged) == 1:\n",
    "        non_dictionary_words_found = [word for word, pos in tagged.items() if dictionary.check(str(word)) is False]\n",
    "    # For multiple words in the string, firstly check for non-noun words (indicating that the POS model cannot categorise the word) and then check if the word exists in the dictionary.\n",
    "    else:\n",
    "        non_dictionary_words_found = [word for word, pos in tagged.items() if pos not in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] and dictionary.check(str(word)) is False]\n",
    "    # If the % of words in the string that arn't in the dictionary are > 1/3, this returns a True <bool> indicating that the string is a serial number\n",
    "    return len(non_dictionary_words_found)/len(tokens) >= 0.33, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "652e5d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XYZ 13423 / ILD\n",
      "(True, 'XYZ 13423 / ILD')\n",
      "\n",
      "ABC/ICL/20891NC\n",
      "(True, 'ABC/ICL/20891NC')\n",
      "\n",
      "stc112hJJ5\n",
      "(True, 'stc112hJJ5')\n",
      "\n",
      "xn256 1jj\n",
      "(True, 'xn256 1jj')\n",
      "\n",
      "Smart Camel\n",
      "(False, 'Smart Camel')\n",
      "\n",
      "Lemon Pancake\n",
      "(False, 'Lemon Pancake')\n"
     ]
    }
   ],
   "source": [
    "for example_serial_number in example_serial_numbers:\n",
    "    print()\n",
    "    print(example_serial_number)\n",
    "    print(serial_number_engine(example_serial_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c4f11f",
   "metadata": {},
   "source": [
    "## Company names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fdb64c",
   "metadata": {},
   "source": [
    "<p>For company names, the spacy named entity model works nicely again to establish ORG named entities. However, as per the spec we have to deal with:<ol><li>Abbreviations</li><li>International entities</li><li>Company structures e.g. inc. ltd. org.</li></ol></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e0035ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name_examples = [\"Marks and Spencers Ltd\", \"M&S Limited\", \"NVIDIA Ireland\", \"Apple Inc.\", \"Vector.ai\", \"Bruce Pannaman\", \"Henry the Hoover\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b41cc12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def company_engine(string):\n",
    "    \"\"\"Used the Spacy Named entity recognition model to assess whether there is an ORG \"\"\"\n",
    "    doc = nlp(company_name_example)\n",
    "    for ent in doc.ents:\n",
    "        # Can the Named Entity Recognition model find an organisation level entity?\n",
    "        if ent.label_ == \"ORG\":\n",
    "            return True, ent.text\n",
    "    return False, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a80bb52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 'Marks')\n",
      "(True, 'M&S Limited')\n",
      "(False, 'NVIDIA Ireland')\n",
      "(True, 'Apple Inc.')\n",
      "(False, 'Vector.ai')\n",
      "(False, 'Bruce Pannaman')\n",
      "(False, 'Henry the Hoover')\n"
     ]
    }
   ],
   "source": [
    "for company_name_example in company_name_examples:\n",
    "    print(company_engine(company_name_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c1b3d",
   "metadata": {},
   "source": [
    "<p>This approach is not perfect as it has missed an entity like NVIDIA. An extra lookup could be used to confirm this using an third-party company data API such as <a href=\"https://docs.intrinio.com/documentation/python/get_company_v2\">Intrinio</a>, which uses data from public listings on stock exchanges to return data around the company. I would look for n-grams that have a Proper Noun followed by a geographic marker to identify a company name such as \"NVIDIA Ireland\" to parse against the API. <br><br>This approach would be good to get additional data to supplement the company once we have grouped them together.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22e89a3",
   "metadata": {},
   "source": [
    "## Grouping entities together in a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f61713",
   "metadata": {},
   "source": [
    "<p>The second part of this test is having the ability to ingest and process strings and not only separate them into the categories above but also group similar entities together. The ways I intend to do this within this task are:</p><ol><li>Manage conjunctions such as \"&\", \"and\", \"-\" etc.</li><li>Handle abbreviations e.g. M&S = Marks and Spencers</li><li>Bring together similar worldwide entities e.g. Amazon Europe Ltd. = Amazon Ltd.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5001df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityEngine:\n",
    "    def __init__(self):\n",
    "        self.entities = {\"company_names\": [], \"company_addresses\": [], \"serial_numbers\": [], \"physical_goods\": [], \"locations\": []}\n",
    "        self.company_stop_words = [\"ltd\", \"inc\", \"org\", \"gmbh\", \"&\"]\n",
    "        self.abbreviation_regex = r\"\\b[A-Z]{2,}\\b\"\n",
    "    \n",
    "    def categorise_entities(self, input_string):\n",
    "        \"\"\"Use each of the engines created above to identify each category and add them to the entities list.\"\"\"\n",
    "        location_found, locations = LocationEngine(input_string).identify_location()\n",
    "        if location_found:\n",
    "            self.handle_duplicate_entities(\"locations\", locations)\n",
    "        \n",
    "        physical_object_found, physical_good = physical_goods_engine(input_string)\n",
    "        if physical_object_found:\n",
    "            self.handle_duplicate_entities(\"physical_goods\", physical_good)\n",
    "        \n",
    "        serial_number_found, serial_number = serial_number_engine(input_string)\n",
    "        if serial_number_found:\n",
    "            self.handle_duplicate_entities(\"serial_numbers\", serial_number)\n",
    "        \n",
    "        company_name_found, company_name = company_engine(company_name_example)\n",
    "        if company_name_found:\n",
    "            self.handle_duplicate_entities(\"company_names\", company_name)\n",
    "        \n",
    "    def handle_duplicate_entities(self, category, string):\n",
    "        \"\"\"Takes each string through the find_similarity() method to try and establish if there is a similar entity already there\"\"\"\n",
    "        # Go through each entity of the found category to see if there is a similarity\n",
    "        for other_entity in self.entities[category]:\n",
    "            if self.find_similarity(other_entity, string):\n",
    "                other_entity += \" + %s\" % str(string)\n",
    "                return\n",
    "        # If no similarity found, add it to the end of the category list\n",
    "        self.entities[category].append(str(string))\n",
    "        \n",
    "    def clean_string(self, string, lower=True):\n",
    "        \"\"\"Removes company and NLTK stop words from the string. Can lowercase too if keyword argument is changed\"\"\"\n",
    "        if lower is True:\n",
    "            return \" \".join([word for word in str(string).lower().split(\" \") if word not in stopwords.words() and word not in self.company_stop_words])\n",
    "        else:\n",
    "            return \" \".join([word for word in str(string).split(\" \") if word not in stopwords.words() and word not in self.company_stop_words])\n",
    "        \n",
    "    \n",
    "    def find_similarity(self, previous_string, new_string):\n",
    "        \"\"\"Goes through some of the similarity edge cases to see of an entity of the same category already exists within the category section of the entity.\"\"\"\n",
    "        previous_string_clean = self.clean_string(previous_string)\n",
    "        new_string_clean = self.clean_string(new_string)\n",
    "        \n",
    "        # Find the same word (excluding stop words) in each entity string\n",
    "        if any([word in new_string_clean.split(\" \") for word in previous_string_clean.split(\" \")]):\n",
    "            return True\n",
    "        \n",
    "        # Look for abbreviations in both previous strings and next strings. ASSUMPTION - abbreviations are typed gramatically and have all caps in the abbreviation\n",
    "        abbreviations_previous_string = re.findall(self.abbreviation_regex, self.clean_string(previous_string, lower=False))\n",
    "        abbreviations_new_string = re.findall(self.abbreviation_regex, self.clean_string(new_string, lower=False))\n",
    "\n",
    "        # abbreviation found in previous string\n",
    "        if len(abbreviations_previous_string) > 0:\n",
    "            new_string_words = self.clean_string(new_string, lower=False).split(\" \")\n",
    "            for i in range(len(new_string_words)):\n",
    "                if i <= len(new_string_words) + len(abbreviations_previous_string[0]):\n",
    "                    if new_string_words[i: i+len(abbreviations_previous_string[0])] == abbreviations_previous_string[0]:\n",
    "                        return True\n",
    "                    \n",
    "        # abbreviation found in new string\n",
    "        elif len(abbreviations_new_string) > 0:\n",
    "            previous_string_words = self.clean_string(previous_string, lower=False).split(\" \")\n",
    "            for i in range(len(previous_string_words)):\n",
    "                if i <= len(previous_string_words) + len(abbreviations_new_string[0]):\n",
    "                    if previous_string_words[i: i+len(abbreviations_new_string[0])] == abbreviations_new_string[0]:\n",
    "                        return True\n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40d70f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_strings = [\"MARKS AND SPENCERS LTD\", \"LONDON\", \"ICNAO02312\", \"LONDON, GREAT BRITAIN\", \"TOYS\", \"INTEL LLC\", \"M&S CORPORATION Limited\", \"LONDON, ENGLAND\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e4f98ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee = EntityEngine()\n",
    "\n",
    "for example_string in example_strings:\n",
    "    ee.categorise_entities(example_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176512eb",
   "metadata": {},
   "source": [
    "### At this point, I have spent a couple of hours on this and will stop here. <br><br>Hopefully from what I have documented here, I have shown my thinking around this problem and highlighted the next steps to make this system better by adding extra use cases and other data sources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
